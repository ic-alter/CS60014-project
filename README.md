# 基于 Qwen + LoRA 的文言文翻译生成模型

## 1. 项目简介（Project Overview）

本项目旨在复现并扩展一个**文本生成模型**，研究在**有限算力条件下**，如何通过参数高效微调方法（LoRA）将通用语言模型适配到特定生成任务。

具体而言，本项目选用 **Qwen2.5-1.5B-Instruct** 作为基础语言模型，结合 **QLoRA（4-bit 量化的 LoRA 微调方法）**，在 **C2MChn 文言文—现代汉语平行语料**上进行微调，完成 **文言文翻译为白话文** 的生成任务。

项目主要包含以下几个步骤：

1. **数据准备**  
   将 C2MChn 数据集中原始的 `.cch`（文言文）和 `.mch`（现代汉语）文件转换为指令微调（instruction tuning）格式的数据。

2. **基础模型加载**  
   下载并本地保存预训练的 Qwen2.5-1.5B-Instruct 模型，作为后续微调和推理的基线模型。

3. **LoRA 微调**  
   在冻结基础模型参数的前提下，仅对少量 LoRA 参数进行训练，使模型更好地适应文言文翻译任务。

4. **推理与对比**  
   使用基础模型与微调后的 LoRA 模型分别对测试集样本进行翻译，生成对比结果，用于定性分析微调前后的效果差异。

本项目的目标是验证：LoRA 能够有效提升小规模语言模型在特定生成任务（文言文翻译）上的表现。

---

## 2. 环境配置与运行方式（Environment Setup）

### 2.1 硬件与系统环境

- GPU：NVIDIA GPU（至少 6GB 显存，已在 RTX 3060 上测试）
- 操作系统：Windows 或 Linux
- Python 版本：Python 3.9 及以上

### 2.2 依赖安装

在项目根目录下，使用如下命令安装依赖：

```bash
pip install -r requirements.txt
```

### 2.3 项目目录结构
```
project/
├── data/                  # 原始数据与处理后的数据
│   └── SCUT-C2MChn/
├── src/                   # 项目源代码
│   ├── prepare_data.py    # 数据预处理
│   ├── download_base.py   # 下载基础模型
│   ├── train_lora.py      # LoRA 微调
│   └── translate_compare.py # 推理与结果对比
├── results/               # 生成的翻译与对比结果
├── requirements.txt
└── README.md
```
(由于原始数据集较大，所以未直接包含在github仓库中，可从https://github.com/Zongyuan-Jiang/C2MChn下载)

### 2.4 运行流程
1. 数据预处理
```
python src/prepare_data.py
```

2. 下载基础模型
```
python src/download_base.py
```

3. LoRA 微调（可根据算力调整训练规模）
```
python src/train_lora.py
```

4. 让base模型和微调模型进行翻译，查看推理效果
```
python src/translate_compare.py
```

## 3. 实验设置（Experimental Setup）

### 3.1 数据集说明

本项目使用 **C2MChn（Classical-to-Modern Chinese）** 文言文翻译数据集。  
该数据集以平行语料的形式提供，每条样本包含一条文言文及其对应的现代汉语翻译。

- `.cch`：文言文原文  
- `.mch`：对应的现代汉语翻译  
- `.domain`：文本来源或领域标签（本实验未使用）

在实验中，数据被划分为：
- **训练集（train）**：用于 LoRA 微调
- **验证集（valid）**：用于训练过程中的简单验证
- **测试集（test）**：仅用于推理与结果对比

原始数据在预处理阶段被转换为指令微调格式：
```
instruction: 将下面文言文翻译成现代汉语
input: <文言文句子>
output: <现代汉语翻译>
```
### 3.2 模型与微调方法

- **基础模型**：Qwen2.5-1.5B-Instruct  
- **模型规模**：约 1.5B 参数  
- **微调方法**：QLoRA（4-bit 量化的 LoRA 微调）

在微调过程中：
- 冻结基础模型的原始参数
- 仅训练新增的 LoRA 低秩适配参数
- 通过 4-bit 量化降低显存占用，使训练可在单张消费级 GPU 上完成

---

### 3.3 训练配置

实验采用以下主要训练设置（可根据算力适当调整）：

- LoRA rank：8  
- LoRA alpha：16  
- 最大序列长度：512  
- 每卡 batch size：1  
- 梯度累积步数：16  
- 训练轮数：1–2 epochs  
- 学习率：2e-4  
- 精度：FP16（配合 4-bit 量化）

该配置在保证训练稳定性的同时，尽量降低计算和显存开销。

---

### 3.4 推理与对比方式

在测试阶段，分别使用：

1. **未微调的基础模型（Base Model）**
2. **经过 LoRA 微调后的模型（LoRA Model）**

对测试集中随机抽取的文言文样本进行翻译。

翻译结果以 JSONL 格式保存，并包含：
- 文言文原文
- 参考的现代汉语翻译
- 基础模型的翻译结果
- LoRA 模型的翻译结果

生成的对比结果用于后续的定性分析，以观察 LoRA 微调对文言文翻译质量的影响。

## 4. 实验结果与分析（Results and Analysis）

本章对比分析基础模型（Base Model）与经过不同轮数 LoRA 微调后的模型在文言文翻译任务上的表现差异。实验主要采用**定性分析**的方式，对生成结果的可读性、贴近程度以及错误类型进行观察和总结。

---

### 4.1 基础模型（Base Model）表现分析

在未进行微调的情况下，Qwen2.5-1.5B-Instruct 已具备一定的文言文理解能力，能够在部分样本中给出大致可理解的翻译结果。然而，其输出仍存在以下明显问题：

1. **输出冗长，偏离翻译任务**  
   基础模型常常在翻译结果中加入解释性文本或主观说明，例如：
   - 对文言文进行“逐词解释”
   - 输出类似“这句话的意思是……”的分析性内容  
   这表明模型未能稳定聚焦于“翻译”这一单一生成目标。

2. **翻译风格不统一**  
   同一类型句子中，部分输出接近直译，部分则偏向意译或解释，整体风格不稳定。

3. **对文言虚词和省略结构处理不佳**  
   对“之、其、而、遂”等文言虚词的处理较为随意，部分句子中存在语义断裂或逻辑不清的问题。

总体来看，基础模型**可以完成任务，但并非针对文言文翻译优化**，仍保留明显的通用对话模型特征。

---

### 4.2 LoRA 微调（2 Epoch）结果分析

在经过 **2 个 epoch 的 LoRA 微调** 后，模型输出发生了明显变化，主要体现在以下方面：

1. **输出更加简洁，贴近参考译文形式**  
   与基础模型相比，LoRA 模型大幅减少了无关解释性文本，输出更接近标准的“文言文 → 白话文”翻译格式。

2. **任务一致性显著提升**  
   微调后的模型基本能够稳定遵循“翻译”指令，不再频繁偏离到解释或分析模式，说明 LoRA 成功强化了模型对特定任务的条件约束。

3. **对常见句式与虚词的处理有所改善**  
   在部分样本中，模型对文言句式的拆解更加自然，白话翻译整体流畅度优于基础模型。

需要注意的是，在 2 epoch 设置下，模型仍存在一定比例的**语义偏移或翻译不完整**现象，但整体趋势已经明显优于基础模型。

---

### 4.3 LoRA 微调（5 Epoch）结果分析

进一步将训练轮数增加至 **5 个 epoch** 后，模型在部分样本上表现出以下特征：

1. **语言流畅度进一步提高**  
   输出在句法和表面语言层面更加通顺，白话文表达更加“自然”。

2. **部分样本出现语义漂移或过拟合倾向**  
   相较于 2 epoch 的结果，5 epoch 微调的模型在少数样本中出现：
   - 偏离原文语义
   - 引入与原文无关的内容
   - 翻译结果看似通顺，但与参考译文不一致  
   这可能与训练轮数增加、模型对小规模数据集过拟合有关。

3. **并非所有样本均优于 2 epoch 结果**  
   在若干例子中，2 epoch 模型的翻译在语义准确性上反而更接近参考译文，说明更长的训练并不一定带来单调提升。

---

### 4.4 综合讨论

综合对比三种模型配置，可以得到如下结论：

- **LoRA 微调显著提升了模型在文言文翻译任务上的一致性与针对性**
- 在有限数据和算力条件下，**适度的训练轮数（如 2 epoch）已能取得较为理想的效果**
- 继续增加训练轮数虽然可能提升语言流畅度，但也可能带来语义偏移和过拟合风险

因此，在本实验设置下，**2 epoch 的 LoRA 微调在效果与效率之间取得了较好的平衡**，验证了参数高效微调方法在小模型和特定生成任务中的实用价值。